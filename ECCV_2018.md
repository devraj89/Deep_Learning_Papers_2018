
[1] Deep Co-Training for Semi-Supervised Image Recognition
-----------------------------------------------------------

http://openaccess.thecvf.com/content_ECCV_2018/papers/Siyuan_Qiao_Deep_Co-Training_for_ECCV_2018_paper.pdf

-- interesting approach called co-training where multiple views of the same data 
are used to learn a semi-supervised model 

-- basically since the data is essentially the same, there is a chance that both the networks will collapse to the
same thing hence they are creating adversarial examples so that the networks provide complementary information
to one-another 

-- basically look at eqn (2) and (5) they are creating a new dataset D' with adversarial examples where the predictions of 
net1 is not equal to that of net2 . it is kind of that adversarial examples of net2 should not hamper the performance of net1.

-- basically WHAT NET 1 MAKES MISTAKES ON should not be CARRIED OVER TO THE MISTAKES OF NET 2

-- creating adversarial examples from one modality to another is a challenge 

[2] HybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning
-------------------------------------------------------------------------------------------

https://github.com/dakshitagrawal97/HybridNet

https://arxiv.org/pdf/1807.11407.pdf

-- an interesting paper in which data is sent through two encoders Ec Eu and two decoders Dc and Du. The Ec and Dc are used for the discriminative pathway whereas the non-discriminative pathway is through Eu and Du.

-- and the outputs of the both Dc and Du should be able to reconstruct x otherwise the two decoders does partial reconstructions

-- check the intermediate reconstruction losses : this helps to make use of the Ec Dc for the reconstruction also and NOT ONLY Eu Du

-- check branch co-operation: loss : BASICALLY WHICHEVER BRANCH IS CONTRIBUTING LESS TO THE RECONSTRUCTION should be penalized
check eqn (6)

-- might be useful to check out the code 

[3] Learning with Biased Complementary Labels
-------------------------------------------------------------------------------------------

http://openaccess.thecvf.com/content_ECCV_2018/papers/Xiyu_Yu_Learning_with_Biased_ECCV_2018_paper.pdf

-- an interesting problem where we define what are complementary labels ? 

-- like the noise adaptation paper in a way 

-- is it useful for any general loss functions ? suppose by studying the data features itself we can generate the complementary labels (or say which class it does not definitely belong to).. can we incorporate penalty if those predictions happen during training ?

-- how to incorporate this into cross modal setting ? 

-- how are complementary labels gathered ?


[4] Semi-Supervised Deep Learning with Memory
-------------------------------------------------------------------------------------------

http://openaccess.thecvf.com/content_ECCV_2018/papers/Yanbei_Chen_Semi-Supervised_Deep_Learning_ECCV_2018_paper.pdf

https://github.com/yanbeic/semi-memory

-- key value and embedding value : key is like the centroid , value is like the average softmax activation vectors 

-- the assimilation and accomodation interaction : when a new data item comes it is matched with the key value pair and then we compute the memory prediction. for the unlabeled data also we use entropy. also the memory prediction should match with that of the network by KL divergence loss

-- my PRL idea is also quite similar to this

[5] Attributes as Operators: Factorizing Unseen Attribute-Object Compositions
-------------------------------------------------------------------------------------------

http://openaccess.thecvf.com/content_ECCV_2018/papers/Tushar_Nagarajan_Attributes_as_Operators_ECCV_2018_paper.pdf

https://arxiv.org/abs/1803.09851

https://github.com/Tushar-N/attributes-as-operators

-- an intersting paper which is able to detect new attribute object compositions basically they like to deterct sliced banana but they might have seen banana and sliced apple separately

-- interestingly they model that we can change the composition -- (a) like remove "peeled" (b) add "ripe" (c) apply multiple attributes etc. check the individual losses which they define with properties like commutaibility and associativity.

-- finally a standard metric learning loss

-- code is in pytorch
