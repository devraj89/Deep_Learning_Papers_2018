
[1] A Two-Step Disentanglement Method
---------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Hadad_A_Two-Step_Disentanglement_CVPR_2018_paper.pdf

-- can be useful for novelty detection


[2] Hierarchical Novelty Detection for Visual Object Recognition
---------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Hierarchical_Novelty_Detection_CVPR_2018_paper.pdf

-- an interesting problem statement

[3] Learning to Compare: Relation Network for Few-Shot Learning
---------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf


[4] HashGAN: Deep Learning to Hash With Pair Conditional Wasserstein GAN
-----------------------------------------------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Cao_HashGAN_Deep_Learning_CVPR_2018_paper.pdf

-- does work with partial data x_i, x_j, s_ij

-- but does not try to figure out the mising relations but rather generates more examples for augmentation purposes

-- the problem that I am trying to solve still remains open


[5] Triplet-Center Loss for Multi-View 3D Object Retrieval
-----------------------------------------------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/He_Triplet-Center_Loss_for_CVPR_2018_paper.pdf

-- triplet loss + center loss 

-- center loss takes care of intra class variance but nothing about inter class

-- be close to the correct center and far away from the nearest center of some other class (nearest negative center)

[6]  Adversarially Learned One-Class Classifier for Novelty Detection CVPR 2018  
-----------------------------------------------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.pdf


[7] Iterative Learning with Open-set Noisy Labels
-----------------------------------------------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Iterative_Learning_With_CVPR_2018_paper.pdf

-- a very interesting problem statement where there are noisy labels

-- but the noisy labels does not mean that the they belong to one of the classes in the dataset in fact it can be an outlier 

-- as an example consider that you have cat dog with clean and noisy labels however you also have images of elephants with suppose dog labels obviously elephant images should not even be there . basically a kind of open set

-- definitely useful for mine own work and supritim's work

-- quick thought ? suppose you use some outlier algorithm and only use the in distribution samples to train your algorithm does the performance in general increase ?

[8] Matching Adversarial Networks
-----------------------------------------------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Mattyus_Matching_Adversarial_Networks_CVPR_2018_paper.pdf

-- an interesting way to model the conditional GAN discriminator architecture

-- basically what it is doing is trying to compute a kind of triplet loss by providing positive pairs and negative pairs ... also while sending the pairs they are actually utilizing the generated examples 

-- not sure how applicable it is to attribute ... as they are obviously using transformations which are only applicable to images 


[9] Deep Mutual Learning
-----------------------------------------------------------------------

http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Deep_Mutual_Learning_CVPR_2018_paper.pdf

-- an interesting problem in which a larger network tries to make a smaller student network mimic its performance or even do better

-- this deals with the fact that a smaller network might be able to achieve the same accuracy as a larger network but might be simply harder to train ... just using supervised loss might not be good enough to easily get those parameters and so a different kind of approach so that the smaller network learns 

-- distillation sounds like a fascinating technique to learn






















