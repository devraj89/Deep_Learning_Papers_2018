
[1] Equal But Not The Same: Understanding the Implicit Relationship Between Persuasive Images and Text 
------------------------------------------------

-- suggested by Siavaram

-- basically they are saying that the two domains do not give redundant information but rather complementary information and hence it is important to consider both of them

-- not like image captioning where basically they just say/read aloud what is given in the image 

[2] VSE++: Improving Visual-Semantic Embeddings with Hard Negatives
---------------------------------------------------------------------

-- We present a new technique for learning visual-semantic embeddings for cross-modal retrieval. Inspired by hard negative mining, the use of hard negatives in structured prediction, and ranking loss functions, we introduce a simple change to common loss functions used for multi-modal embeddings. That, combined with fine-tuning and use of augmented data, yields significant gains in retrieval performance. We showcase our approach, VSE++, on MS-COCO and Flickr30K datasets, using ablation studies and comparisons with existing methods. On MS-COCO our approach outperforms state-ofthe-art methods by 8.8% in caption retrieval and 11.3% in image retrieval (at R@1). 

-- another version of negative mining (introduces novelty into the triplet loss)

[3] Semantic Embedding for Sketch-Based 3D Shape Retrieval 
-----------------------------------------------------------

-- a simple paper for cross modal retrieval 

-- triplet loss but only sketch and 3d shape 

[4] Metric Learning for Novelty and Anomaly Detection
-----------------------------------------------------------

-- When neural networks process images which do not resemble the distribution seen during training, so called out-of-distribution images, they often make wrong predictions, and do so too confidently. The capability to detect out-of-distribution images is therefore crucial for many real-world applications. We divide out-of-distribution detection between novelty detection —images of classes which are not in the training set but are related to those—, and anomaly detection —images with classes which are unrelated to the training set. By related we mean they contain the same type of objects, like digits in MNIST and SVHN. Most existing work has focused on anomaly detection, and has addressed this problem considering networks trained with the cross-entropy loss. Differently from them, we propose to use metric learning which does not have the drawbackof the softmax layer (inherent to cross-entropy methods), which forces the network to divide its prediction power over the learned classes. We perform extensive experiments and evaluate both novelty and anomaly detection, even in a relevant application such as traffic sign recognition, obtaining comparable or better results than previous works. 

[5] Resembled Generative Adversarial Networks: Two Domains with Similar Attributes
-----------------------------------------------------------

-- We propose a novel algorithm, namely Resembled Generative Adversarial Networks (GAN), that generates two different domain data simultaneously where they resemble each other. Although recent GAN algorithms achieve the great success in learning the cross-domain relationship [9, 19, 22], their application is limited to domain transfers, which requires the input image. The first attempt to tackle the data generation of two domains was proposed by CoGAN [10]. However, their solution is inherently vulnerable for various levels of domain similarities. Unlike CoGAN, our Resembled GAN implicitly induces two generators to match feature covariance from both domains, thus leading to share semantic attributes. Hence, we effectively handle a wide range of structural and semantic similarities between various two domains. Based on experimental analysis on various datasets, we verify that the proposed algorithm is effective for generating two domains with similar attributes

-- they resemble each other ? in case of rgb/flow features or something like image/text they should not resemble each other

-- application is limited to domain transfers not modalities

-- also refer to this paper https://pdfs.semanticscholar.org/4248/12cef92a364e03222488ba246e8c3b1f06a3.pdf this is the first of its kind in this domain 

-- basic idea is that they send the generated samples and the real samples through an pre-trained AE and what the AE gives is like a latent representation. There are two extra discriminators Dxf and Dyf distinguishes the real feature distribution from the fake feature distribution.

-- Inspired by this observation, we regard the mean of feature distribution as domain identity attribute, and the covariance of that as shareable attributes.

-- a new loss term Lfc that represents the norm between the feature covariance matrix of x and that of y. This helps to preserve the feature covariances of the original data and the generated data in the latent space of the AE. To me eqn (3) looks more like a centerine loss if expectation is considered.

-- THIS IS VERY IMPORTANT. Using all samples from both domains as training set, we can ensure that the AE can encode all samples into the same feature space; two feature distributions from both domains can be comparable. THEY CAN ONLY USE A COMMON AE BEACUSE THIS IS DOMAIN DIFFERENCE !

-- NO CHOICE OF CONDITIONAL ATTRIBUTES

[6] Ranking CGANs: Subjective Control over Semantic Image Attributes
-----------------------------------------------------------

-- pytorch code https://github.com/saquil/RankCGAN

-- In this paper, we investigate the use of generative adversarial networks in the task of image generation according to subjective measures of semantic attributes. Unlike the standard (CGAN) that generates images from discrete categorical labels, our architecture handles both continuous and discrete scales. Given pairwise comparisons of images, our model, called RankCGAN, performs two tasks: it learns to rank images using a subjective measure; and it learns a generative model that can be controlled by that measure. RankCGAN associates each subjective measure of interest to a distinct dimension of some latent space. We perform experiments on UT-Zap50K, PubFig and OSR datasets and demonstrate that the model is expressive and diverse enough to conduct two-attribute exploration and image editing. 

-- the idea is very good but not done for two different modalities . It can generate images with multiple difference in attributes. In fact the attribute vector is actually treated as either single label or multi-label attribute. they have used a ranking function to do that.

[7] Cross-Class Sample Synthesis for Zero-shot Learning
-----------------------------------------------------------

-- Zero-shot learning (ZSL) aims to recognize unseen classes which have no available training samples, through establishing an association with seen classes. Existing approaches mostly learn a comparability function to predict the class of an image. Different from previous approaches, we put forward a novel method, Cross-Class Sample Synthesis (CCSS), to directly synthesize samples of unseen classes from specific seen classes in the visual feature space. We adopt class-graph to measure inter-class similarity and propose class entropy to select classes as the synthesis source of target classes. An endto-end network is constructed to realize sample synthesis from source classes to target classes. Specially, rule of attribute guiding cross-class transfer is built into the network, to which various samples of different source classes can be used to synthesize samples of each target class according. The synthesized samples are used as training data of unseen classes and it turns ZSL into a supervised learning problem. Experiments on five benchmark datasets efficiently demonstrate the advantage of our proposed method. 

-- to directly synthesize samples of unseen classes from specific seen classes in the visual feature space.(interesting idea is to select specific classes to synthesise examples)

[8] GAN-based Semi-supervised Learning On Fewer Labeled Samples
-----------------------------------------------------------

-- Semi-supervised learning is recently addressed by means of neural networks in the framework of deep learning. For the semi-supervised tasks where training samples arepartially labeled, the generative adversarial networks (GANs) are applicable not only to  augmentation of the training samples but also to the end-to-end learning of classifiers exploiting the unlabeled samples. It, however, is found that the previous GAN-based semi-supervised method is less effective on the smaller number of labeled samples, and thus in this paper, we propose a novel GAN-based method to effectively work on fewerlabeled samples. In the GAN framework, through analyzing gradients of the discriminator which are fundamental to learn the network via back-propagation, we formulate a discriminator model and accordingly a generator loss to cope with the less discriminative classifier trained on the fewer labeled samples. The proposed model is also mixed with the original one to further improve discriminativity on the semi-supervised learning in an efficient way beyond the simple linear combination. The experimental results on semisupervisedclassification tasks using MNIST, SVHN and CIFAR-10 datasets show that the proposed method exhibits favorable performance compared to the other methods. 

